{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c61677",
   "metadata": {},
   "source": [
    "# Chapter 1. Agents\n",
    "\n",
    "Agents are LLMs that are able to make decisions and take actions, i.e. interact with external environment. Simple RAG is not an Agent because it does not have reasoning or tools to take action. But RAG would decrease latency.\n",
    "\n",
    "\"If you primarily need natural language question‐answering over a corpus, choose a traditional chatbot or RAG architecture. But if you face high variability, open‐ended reasoning, dynamic planning needs, or continual learning requirements, invest in an autonomous agent.\"\n",
    "\n",
    "* autonomous model selection: routing simple queris to more efficient models\n",
    "\n",
    "* asynchronous operations: which tasks or sub-tasks can be done async in Glucoza?\n",
    "\n",
    "* comprehensive documentation: experiments with architectures, workflows, prompts, etc. have to be documented very well\n",
    "\n",
    "* CrewAI + ADK faster for prototypying, but nothing is as customizable and observable as LangChain\n",
    "\n",
    "Principles of Building Effective Agentic Systems:\n",
    "\n",
    "1. Scalability (+workload +throughput +tasks)\n",
    "2. Modularity (well-defined parts connected through clear interface)\n",
    "3. Resilience\n",
    "4. Interoperability/Future-proofing\n",
    "5. Reinforcement (Active/Continuous) Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7f02c",
   "metadata": {},
   "source": [
    "Business and Ethics Risks associated with AI Product in our case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7e6ae",
   "metadata": {},
   "source": [
    "# Chapter 2. Designing Systems\n",
    "\n",
    "[Batch evaluation script](https://github.com/MichaelAlbada/BuildingApplicationsWithAIAgents/blob/main/src/common/evaluation/batch_evaluation.py)\n",
    "\n",
    "You can measure tool precision, parameter accuracy, and overall task success rates across hundreds of examples to catch edge cases before deploying.\n",
    "\n",
    "* tool recall\n",
    "* parameter accuracy\n",
    "* confirmation quality\n",
    "\n",
    "DeepSeek is open-source, can be private, great reasoning, second cheapest after Meta's LLama 3.1 charging around $0.50 per million tokens (gemini is 4 times more expensive, openAI is 3 times more expensive)\n",
    "\n",
    "For scalability, engineering should account for:\n",
    "\n",
    "* bottlenecks\n",
    "* underutilization\n",
    "* rising operational costs\n",
    "* dynamic GPU allocation (based on demand) + load balancing\n",
    "* latency in large MAS (esp in real-time environments)\n",
    "* fault tolerance: making sure erros are detected and the system if recovered gracefully (+redundancy)\n",
    "* consistency and robustness (extensive monitoring and HITL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c445713",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatOpenAI' from 'langchain.chat_models' (/Users/alexxela/.pyenv/versions/3.12.9/envs/langchain-env/lib/python3.12/site-packages/langchain/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SystemMessage, HumanMessage, AIMessage\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolMessage\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ChatOpenAI' from 'langchain.chat_models' (/Users/alexxela/.pyenv/versions/3.12.9/envs/langchain-env/lib/python3.12/site-packages/langchain/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# -- 1) Define our single business tool\n",
    "@tool\n",
    "def cancel_order(order_id: str) -> str:\n",
    "    \"\"\"Cancel an order that hasn't shipped.\"\"\"\n",
    "    # (Here you'd call your real backend API)\n",
    "    return f\"Order {order_id} has been cancelled.\"\n",
    "# -- 2) The agent \"brain\": invoke LLM, run tool, then invoke LLM again\n",
    "def call_model(state):\n",
    "    msgs = state[\"messages\"]\n",
    "    order = state.get(\"order\", {\"order_id\": \"UNKNOWN\"})\n",
    "    # System prompt tells the model exactly what to do\n",
    "    prompt = (\n",
    "    f'''You are an ecommerce support agent.\n",
    "    ORDER ID: {order['order_id']}\n",
    "    If the customer asks to cancel, call cancel_order(order_id)\n",
    "    and then send a simple confirmation.\n",
    "    Otherwise, just respond normally.'''\n",
    "    )\n",
    "    full = [SystemMessage(prompt)] + msgs\n",
    "    # 1st LLM pass: decides whether to call our tool\n",
    "    AIMessage = ChatOpenAI(model=\"gpt-5\", temperature=0)(full)\n",
    "    out = [first]\n",
    "    if getattr(first, \"tool_calls\", None):\n",
    "        # run the cancel_order tool\n",
    "        tc = first.tool_calls[0]\n",
    "        result = cancel_order(**tc[\"args\"])\n",
    "        out.append(ToolMessage(content=result, tool_call_id=tc[\"id\"]))\n",
    "        # 2nd LLM pass: generate the final confirmation text\n",
    "        AIMessage = ChatOpenAI(model=\"gpt-5\", temperature=0)(full + out)\n",
    "        out.append(second)\n",
    "    return {\"messages\": out}\n",
    "# -- 3) Wire it all up in a StateGraph\n",
    "def construct_graph():\n",
    "    g = StateGraph({\"order\": None, \"messages\": []})\n",
    "    g.add_node(\"assistant\", call_model)\n",
    "    g.set_entry_point(\"assistant\")\n",
    "    return g.compile()\n",
    "graph = construct_graph()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_order = {\"order_id\": \"A12345\"}\n",
    "    convo = [HumanMessage(content=\"Please cancel my order A12345.\")]\n",
    "    result = graph.invoke({\"order\": example_order, \"messages\": convo})\n",
    "    for msg in result[\"messages\"]:\n",
    "        print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209931fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal evaluation check\n",
    "example_order = {\"order_id\": \"B73973\"}\n",
    "convo = [HumanMessage(content='''Please cancel order #B73973. I found a cheaper option elsewhere.''')]\n",
    "result = graph.invoke({\"order\": example_order, \"messages\": convo})\n",
    "\n",
    "assert any(\"cancel_order\" in str(m.content) for m in result[\"messages\"], \"Cancel order tool not called\")\n",
    "assert any(\"cancelled\" in m.content.lower() for m in result[\"messages\"], \"Confirmation message missing\")\n",
    "\n",
    "print(\"✅ Agent passed minimal evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f903c48",
   "metadata": {},
   "source": [
    "# Chapter 3. UX Design\n",
    "\n",
    "**CONTEXT MANAGEMENT**: how the agent adapts and persists depending on user's needs and workflow\n",
    "\n",
    "* how the context is managed over time: what people exepct/need to be remembered about them for the Agent to be effective?\n",
    "* communicating agent limitations and capabilities (e.g. suggestions of tasks or commands + menus)\n",
    "* agents must be able to communicate that themselves + dynamically suggest options relevant to the context\n",
    "* users don't think about modalities, they just want their task done quickly and effortlessly\n",
    "* amplify user's capabilities in an elegant way\n",
    "* async implementation to save user's attention when not needed\n",
    "* Proactive vs. Intrusive: depends on context awareness and user control\n",
    "* Effective **STATE** management: server-side memory vs. client-side (browser); log in vs. anonymous (cookies); user-based memory vs. session-based (ephemeral)\n",
    "* failing gracefully: not asking user start from scratch if the agent fails to fulfill the task + _learn from failures (logging)_\n",
    "\n",
    "*\"Reliability begins with consistency\"* -- test if the agents generate similar outputs from the same or similar inputs and ocntexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209616d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
